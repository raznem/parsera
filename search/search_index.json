{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Parsera","text":"<p>Parsera is a lightweight Python library for scraping websites with LLMs. </p> <p>There are 2 ways of using Parsera:  </p> <ul> <li>Install the library and run it locally, it is great for smaller-scale extraction and experiments.</li> <li>Use an API that provides a more scalable way of data extraction out of the box. Also, it contains some extra features like a built-in proxy. </li> </ul>"},{"location":"#community","title":"Community","text":"<p>If you like this project star it on GitHub and join our discussions on Discord server.</p>"},{"location":"#contributors","title":"Contributors","text":"<p>If you are considering contributing to Parsera, check out the guidelines to get started.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thanks for considering contributing to Parsera!  This project is in the early stage of development, so any help will be highly appreciated. You can start from looking through existing issues, or directly asking about the most helpful contributions on Discord.</p>"},{"location":"contributing/#issues","title":"Issues","text":"<p>The best way to ask a question, report a bug, or submit feature request is to submit an Issue. It's much better than asking about it in email or Discord since conversation becomes publicly available and easy to navigate.</p>"},{"location":"contributing/#pull-requests","title":"Pull requests","text":""},{"location":"contributing/#installation-and-setup","title":"Installation and setup","text":"<p>Fork the repository on GitHub and clone your fork locally.  </p> <p>Next, install dependencies using poetry: <pre><code># Clone your fork and cd into the repo directory\ngit clone git@github.com:&lt;your username&gt;/parsera.git\ncd parsera\n\n# If you don't have poetry install it first:\n# https://python-poetry.org/docs/\n# Then:\npoetry install\n# If you are using VS Code you can get python venv path to switch:\npoetry which python\n# To activate virtual environment with installation run:\npoetry shell\n</code></pre> Now you have a virtual environment with Parsera and all necessary dependencies installed.</p>"},{"location":"contributing/#code-style","title":"Code style","text":"<p>The project uses <code>black</code> and <code>isort</code> for formatting. Set up them in your IDE or run this before committing: <pre><code>make format\n</code></pre></p>"},{"location":"contributing/#commit-and-push-changes","title":"Commit and push changes","text":"<p>Commit your changes and push them to your fork, then create a pull request to the Parsera's repository.</p> <p>Thanks a lot for helping improve Parsera!</p>"},{"location":"getting-started/","title":"Welcome to Parsera","text":"<p>Parsera is a lightweight Python library for scraping websites with LLMs. You can run clone and run it locally or use an API, which provides more scalable way and some extra features like built-in proxy.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code>pip install parsera\nplaywright install\n</code></pre>"},{"location":"getting-started/#basic-usage","title":"Basic usage","text":"<p>If you want to use OpenAI, remember to set up <code>OPENAI_API_KEY</code> env variable. You can do this from python with: <pre><code>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY_HERE\"\n</code></pre></p> <p>Next, you can run a basic version that uses <code>gpt-4o-mini</code> <pre><code>from parsera import Parsera\n\nurl = \"https://news.ycombinator.com/\"\nelements = {\n    \"Title\": \"News title\",\n    \"Points\": \"Number of points\",\n    \"Comments\": \"Number of comments\",\n}\n\nscraper = Parsera()\nresult = scraper.run(url=url, elements=elements)\n</code></pre></p> <p><code>result</code> variable will contain a json with a list of records: <pre><code>[\n   {\n      \"Title\":\"Hacking the largest airline and hotel rewards platform (2023)\",\n      \"Points\":\"104\",\n      \"Comments\":\"24\"\n   },\n    ...\n]\n</code></pre></p> <p>There is also <code>arun</code> async method available: <pre><code>result = await scrapper.arun(url=url, elements=elements)\n</code></pre></p>"},{"location":"getting-started/#more-features","title":"More features","text":"<p>Check out further documentation to explore more features:</p> <ul> <li>Running custom models</li> <li>Using proxy</li> <li>Run custom playwright</li> </ul>"},{"location":"api/getting-started/","title":"Getting started","text":""},{"location":"features/custom-models/","title":"Custom models","text":""},{"location":"features/custom-models/#run-with-custom-model","title":"Run with custom model","text":"<p>You can instantiate <code>Parsera</code> with any chat model supported by LangChain, for example, to run the model from Azure: <pre><code>import os\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.getenv(\"AZURE_GPT_BASE_URL\"),\n    openai_api_version=\"2023-05-15\",\n    deployment_name=os.getenv(\"AZURE_GPT_DEPLOYMENT_NAME\"),\n    openai_api_key=os.getenv(\"AZURE_GPT_API_KEY\"),\n    openai_api_type=\"azure\",\n    temperature=0.0,\n)\n\nurl = \"https://news.ycombinator.com/\"\nelements = {\n    \"Title\": \"News title\",\n    \"Points\": \"Number of points\",\n    \"Comments\": \"Number of comments\",\n}\nscrapper = Parsera(model=llm)\nresult = scrapper.run(url=url, elements=elements)\n</code></pre></p>"},{"location":"features/custom-models/#run-local-model-with-trasformers","title":"Run local model with <code>Trasformers</code>","text":"<p>Currently, we only support models that include a <code>system</code> token</p> <p>You should install <code>Transformers</code> with either <code>pytorch</code> (recommended) or <code>TensorFlow 2.0</code></p> <p>Transformers Installation Guide</p> <p>Example: <pre><code>from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom parsera.engine.model import HuggingFaceModel\nfrom parsera import Parsera\n\n# Define the URL and elements to scrape\nurl = \"https://news.ycombinator.com/\"\nelements = {\n\"Title\": \"News title\",\n\"Points\": \"Number of points\",\n\"Comments\": \"Number of comments\",\n}\n\n# Initialize model with transformers pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", trust_remote_code=True)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=5000)\n\n# Initialize HuggingFaceModel\nllm = HuggingFaceModel(pipeline=pipe)\n\n# Scrapper with HuggingFace model\nscrapper = Parsera(model=llm)\nresult = scrapper.run(url=url, elements=elements)\n</code></pre></p>"},{"location":"features/custom-playwright/","title":"Custom playwright","text":""},{"location":"features/custom-playwright/#parserascript","title":"ParseraScript","text":"<p>With <code>ParseraScript</code> class you can execute custom playwright scripts during scraping. There are 2 types of code you can run:</p> <ul> <li><code>initial_script</code> which is executed during the first run of <code>ParseraScript</code>, useful when you need to log in to access the data.</li> <li><code>playwright_script</code> which runs during every <code>run</code> call, which allows to do custom actions before data is extracted, useful when data is hidden behind some button.</li> </ul>"},{"location":"features/custom-playwright/#example-log-in-and-load-data","title":"Example: log in and load data","text":"<p>You can log in to parsera.org and get credits amount with the following code: <pre><code>from playwright.async_api import Page\nfrom parsera import ParseraScript\n\n# Define the script to execute during the session creation\nasync def initial_script(page: Page) -&gt; Page:\n    await page.goto(\"https://parsera.org/auth/sign-in\")\n    await page.wait_for_load_state(\"networkidle\")\n    await page.get_by_label(\"Email\").fill(EMAIL)\n    await page.get_by_label(\"Password\").fill(PASSWORD)\n    await page.get_by_role(\"button\", name=\"Sign In\", exact=True).click()\n    await page.wait_for_selector(\"text=Playground\")\n    return page\n\n# This script is executed after the url is opened\nasync def repeating_script(page: Page) -&gt; Page:\n    await page.wait_for_timeout(1000)  # Wait one second for page to load\n    return page\n\nparsera = ParseraScript(model=model, initial_script=initial_script)\nresult = await parsera.arun(\n    url=\"https://parsera.org/app\",\n    elements={\n        \"credits\": \"number of credits\",\n    },\n    playwright_script=repeating_script,\n)\n</code></pre></p>"},{"location":"features/custom-playwright/#access-playwright-instance","title":"Access Playwright instance","text":"<p>The page is fetched via the <code>ParseraScript.loader</code>, which contains the playwright instance. <pre><code>from parsera import ParseraScript\n\nparsera = ParseraScript(model=model)\n\n## You can manually initialize playwright session and modify it:\nawait parsera.new_session()\nawait parsera.loader.load_content(url=url)\n\n## After page is loaded you can access playwright elements, like Page:\nparsera.loader.page.getByRole('button').click()\n\n## Next you cun run extraction process\nresult = await parsera.arun(\n    url=extraction_url,\n    elements=elements_dict,\n)\n</code></pre></p>"},{"location":"features/proxy/","title":"Proxy","text":""},{"location":"features/proxy/#using-proxy","title":"Using proxy","text":"<p>You can use serve the traffic via proxy server when calling <code>run</code> method: <pre><code>proxy_settings = {\n    \"server\": \"https://1.2.3.4:5678\",\n    \"username\": &lt;PROXY_USERNAME&gt;,\n    \"password\": &lt;PROXY_PASSWORD&gt;,\n}\nresult = scrapper.run(url=url, elements=elements, proxy_settings=proxy_settings)\n</code></pre></p> <p>Where <code>proxy_settings</code> contains your proxy credentials.</p>"}]}