{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Parsera","text":"<p>Parsera is a lightweight Python library for scraping websites with LLMs. </p> <p>There are 2 ways of using Parsera:  </p> <ul> <li>Install the library and run it locally, it is great for smaller-scale extraction and experiments.</li> <li>Use an API that provides a more scalable way of data extraction out of the box. Also, it contains some extra features like a built-in proxy. </li> </ul>"},{"location":"#community","title":"Community","text":"<p>If you like this project star it on GitHub and join our discussions on Discord server.</p>"},{"location":"#contributors","title":"Contributors","text":"<p>If you are considering contributing to Parsera, check out the guidelines to get started.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thanks for considering contributing to Parsera!  This project is in the early stage of development, so any help will be highly appreciated. You can start from looking through existing issues, or directly asking about the most helpful contributions on Discord.</p>"},{"location":"contributing/#issues","title":"Issues","text":"<p>The best way to ask a question, report a bug, or submit feature request is to submit an Issue. It's much better than asking about it in email or Discord since conversation becomes publicly available and easy to navigate.</p>"},{"location":"contributing/#pull-requests","title":"Pull requests","text":""},{"location":"contributing/#installation-and-setup","title":"Installation and setup","text":"<p>Fork the repository on GitHub and clone your fork locally.  </p> <p>Next, install dependencies using poetry: <pre><code># Clone your fork and cd into the repo directory\ngit clone git@github.com:&lt;your username&gt;/parsera.git\ncd parsera\n\n# If you don't have poetry install it first:\n# https://python-poetry.org/docs/\n# Then:\npoetry install\n# If you are using VS Code you can get python venv path to switch:\npoetry which python\n# To activate virtual environment with installation run:\npoetry shell\n</code></pre> Now you have a virtual environment with Parsera and all necessary dependencies installed.</p>"},{"location":"contributing/#code-style","title":"Code style","text":"<p>The project uses <code>black</code> and <code>isort</code> for formatting. Set up them in your IDE or run this before committing: <pre><code>make format\n</code></pre></p>"},{"location":"contributing/#commit-and-push-changes","title":"Commit and push changes","text":"<p>Commit your changes and push them to your fork, then create a pull request to the Parsera's repository.</p> <p>Thanks a lot for helping improve Parsera!</p>"},{"location":"getting-started/","title":"Welcome to Parsera","text":"<p>Parsera is a lightweight Python library for scraping websites with LLMs. You can clone and run it locally or use an API, which provides more scalable way and some extra features like built-in proxy.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code>pip install parsera\nplaywright install\n</code></pre>"},{"location":"getting-started/#basic-usage","title":"Basic usage","text":"<p>If you want to use OpenAI, remember to set up <code>OPENAI_API_KEY</code> env variable. You can do this from python with: <pre><code>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY_HERE\"\n</code></pre></p> <p>Next, you can run a basic version that uses <code>gpt-4o-mini</code> <pre><code>from parsera import Parsera\n\nurl = \"https://news.ycombinator.com/\"\nelements = {\n    \"Title\": \"News title\",\n    \"Points\": \"Number of points\",\n    \"Comments\": \"Number of comments\",\n}\n\nscraper = Parsera()\nresult = scraper.run(url=url, elements=elements)\n</code></pre></p> <p><code>result</code> variable will contain a json with a list of records: <pre><code>[\n   {\n      \"Title\":\"Hacking the largest airline and hotel rewards platform (2023)\",\n      \"Points\":\"104\",\n      \"Comments\":\"24\"\n   },\n    ...\n]\n</code></pre></p> <p>There is also <code>arun</code> async method available: <pre><code>result = await scrapper.arun(url=url, elements=elements)\n</code></pre></p>"},{"location":"getting-started/#running-with-cli","title":"Running with CLI","text":"<p>Before you run <code>Parsera</code> as command line tool don't forget to put your <code>OPENAI_API_KEY</code> to env variables or <code>.env</code> file</p>"},{"location":"getting-started/#usage","title":"Usage","text":"<p>You can configure elements to parse using <code>JSON string</code> or <code>FILE</code>. Optionally, you can provide <code>FILE</code> to write output.</p> <pre><code>python -m parsera.main URL {--scheme '{\"title\":\"h1\"}' | --file FILENAME} [--output FILENAME]\n</code></pre>"},{"location":"getting-started/#more-features","title":"More features","text":"<p>Check out further documentation to explore more features:</p> <ul> <li>Running custom models</li> <li>Using proxy</li> <li>Run custom playwright</li> <li>Extractors</li> <li>Docker</li> </ul>"},{"location":"api/getting-started/","title":"Getting started","text":"<p>First, go to Parsera web page and generate an API key.</p>"},{"location":"api/getting-started/#extract-endpoint","title":"Extract endpoint","text":"<p>Paste this key to <code>X-API-KEY</code> header to send the request to <code>extract</code> endpoint: <pre><code>curl https://api.parsera.org/v1/extract \\\n--header 'Content-Type: application/json' \\\n--header 'X-API-KEY: &lt;YOUR_API_KEY&gt;' \\\n--data '{\n    \"url\": \"https://news.ycombinator.com/\",\n    \"attributes\": [\n        {\n            \"name\": \"Title\",\n            \"description\": \"News title\"\n        },\n        {\n            \"name\": \"Points\",\n            \"description\": \"Number of points\"\n        }\n    ],\n    \"proxy_country\": \"UnitedStates\"\n}'\n</code></pre></p> <p>By default, <code>proxy_country</code> is <code>UnitedStates</code>, it's recommended to set <code>proxy_country</code> parameter to a specific country in the request since a page could not be available from all locations. Here you can find a full list of proxy countries available.</p>"},{"location":"api/getting-started/#parse-endpoint","title":"Parse endpoint","text":"<p>In addition to <code>extract</code>, there is a <code>parse</code> endpoint that can be used to parse data generated on your side instead of one from url. There is a <code>content</code> attribute for passing data, which accepts both raw html and string: <pre><code>curl https://api.parsera.org/v1/parse \\\n--header 'Content-Type: application/json' \\\n--header 'X-API-KEY: &lt;YOUR_API_KEY&gt;' \\\n--data '{\n    \"content\": &lt;HTML_OR_TEXT_HERE&gt;,\n    \"attributes\": [\n        {\n            \"name\": \"Title\",\n            \"description\": \"News title\"\n        },\n        {\n            \"name\": \"Points\",\n            \"description\": \"Number of points\"\n        }\n    ],\n}'\n</code></pre></p>"},{"location":"api/getting-started/#swagger-doc","title":"Swagger doc","text":"<p>You can also explore Swagger doc of the API following this link: https://api.parsera.org/docs#/.</p>"},{"location":"api/proxy/","title":"Proxy","text":""},{"location":"api/proxy/#setting-proxy-country","title":"Setting proxy country","text":"<p>You can use the <code>proxy_country</code> parameter to set a proxy country. The default is <code>UnitedStates</code>, and it's recommended to change it since your page could not be available from all locations.</p> <p>To scrape the page from the United States you have to set <code>proxy_country</code> to <code>UnitedStates</code>: <pre><code>curl https://api.parsera.org/v1/extract \\\n--header 'Content-Type: application/json' \\\n--header 'X-API-KEY: &lt;YOUR-API-KEY&gt;' \\\n--data '{\n    \"url\": &lt;TARGET-URL&gt;,\n    \"attributes\": [\n        {\n            \"name\": &lt;First attribute name&gt;,\n            \"description\": &lt;First attribute description&gt;,\n        },\n        {\n            \"name\": &lt;Second attribute name&gt;,\n            \"description\": &lt;Second attribute description&gt;\n        }\n    ],\n    \"proxy_country\": \"UnitedStates\"\n}'\n</code></pre></p>"},{"location":"api/proxy/#list-of-proxy-countries","title":"List of proxy countries","text":"<p>Send a <code>GET</code> request to this URL https://api.parsera.org/v1/proxy-countries, to get the list of countries programmatically.</p> <p>Here is the list of countries available:</p> <ul> <li>Random Country - <code>random</code></li> <li>Afghanistan - <code>Afghanistan</code></li> <li>Albania - <code>Albania</code></li> <li>Algeria - <code>Algeria</code></li> <li>Argentina - <code>Argentina</code></li> <li>Armenia - <code>Armenia</code></li> <li>Aruba - <code>Aruba</code></li> <li>Australia - <code>Australia</code></li> <li>Austria - <code>Austria</code></li> <li>Azerbaijan - <code>Azerbaijan</code></li> <li>Bahamas - <code>Bahamas</code></li> <li>Bahrain - <code>Bahrain</code></li> <li>Bangladesh - <code>Bangladesh</code></li> <li>Belarus - <code>Belarus</code></li> <li>Belgium - <code>Belgium</code></li> <li>Bosnia and Herzegovina - <code>BosniaandHerzegovina</code></li> <li>Brazil - <code>Brazil</code></li> <li>British Virgin Islands - <code>BritishVirginIslands</code></li> <li>Brunei - <code>Brunei</code></li> <li>Bulgaria - <code>Bulgaria</code></li> <li>Cambodia - <code>Cambodia</code></li> <li>Cameroon - <code>Cameroon</code></li> <li>Canada - <code>Canada</code></li> <li>Chile - <code>Chile</code></li> <li>China - <code>China</code></li> <li>Colombia - <code>Colombia</code></li> <li>Costa Rica - <code>CostaRica</code></li> <li>Croatia - <code>Croatia</code></li> <li>Cuba - <code>Cuba</code></li> <li>Cyprus - <code>Cyprus</code></li> <li>Chechia - <code>Chechia</code></li> <li>Denmark - <code>Denmark</code></li> <li>Dominican Republic - <code>DominicanRepublic</code></li> <li>Ecuador - <code>Ecuador</code></li> <li>Egypt - <code>Egypt</code></li> <li>El Salvador - <code>ElSalvador</code></li> <li>Estonia - <code>Estonia</code></li> <li>Ethiopia - <code>Ethiopia</code></li> <li>Finland - <code>Finland</code></li> <li>France - <code>France</code></li> <li>Georgia - <code>Georgia</code></li> <li>Germany - <code>Germany</code></li> <li>Ghana - <code>Ghana</code></li> <li>Greece - <code>Greece</code></li> <li>Guatemala - <code>Guatemala</code></li> <li>Guyana - <code>Guyana</code></li> <li>Hashemite Kingdom of Jordan - <code>HashemiteKingdomofJordan</code></li> <li>Hong Kong - <code>HongKong</code></li> <li>Hungary - <code>Hungary</code></li> <li>India - <code>India</code></li> <li>Indonesia - <code>Indonesia</code></li> <li>Iran - <code>Iran</code></li> <li>Iraq - <code>Iraq</code></li> <li>Ireland - <code>Ireland</code></li> <li>Israel - <code>Israel</code></li> <li>Italy - <code>Italy</code></li> <li>Jamaica - <code>Jamaica</code></li> <li>Japan - <code>Japan</code></li> <li>Kazakhstan - <code>Kazakhstan</code></li> <li>Kenya - <code>Kenya</code></li> <li>Kosovo - <code>Kosovo</code></li> <li>Kuwait - <code>Kuwait</code></li> <li>Latvia - <code>Latvia</code></li> <li>Liechtenstein - <code>Liechtenstein</code></li> <li>Luxembourg - <code>Luxembourg</code></li> <li>Macedonia - <code>Macedonia</code></li> <li>Madagascar - <code>Madagascar</code></li> <li>Malaysia - <code>Malaysia</code></li> <li>Mauritius - <code>Mauritius</code></li> <li>Mexico - <code>Mexico</code></li> <li>Mongolia - <code>Mongolia</code></li> <li>Montenegro - <code>Montenegro</code></li> <li>Morocco - <code>Morocco</code></li> <li>Mozambique - <code>Mozambique</code></li> <li>Myanmar - <code>Myanmar</code></li> <li>Nepal - <code>Nepal</code></li> <li>Netherlands - <code>Netherlands</code></li> <li>New Zealand - <code>NewZealand</code></li> <li>Nigeria - <code>Nigeria</code></li> <li>Norway - <code>Norway</code></li> <li>Oman - <code>Oman</code></li> <li>Pakistan - <code>Pakistan</code></li> <li>Palestine - <code>Palestine</code></li> <li>Panama - <code>Panama</code></li> <li>Papua New Guinea - <code>PapuaNewGuinea</code></li> <li>Paraguay - <code>Paraguay</code></li> <li>Peru - <code>Peru</code></li> <li>Philippines - <code>Philippines</code></li> <li>Poland - <code>Poland</code></li> <li>Portugal - <code>Portugal</code></li> <li>Puerto Rico - <code>PuertoRico</code></li> <li>Qatar - <code>Qatar</code></li> <li>Republic of Lithuania - <code>RepublicOfLithuania</code></li> <li>Republic of Moldova - <code>RepublicOfMoldova</code></li> <li>Romania - <code>Romania</code></li> <li>Russia - <code>Russia</code></li> <li>Saudi Arabia - <code>SaudiArabia</code></li> <li>Senegal - <code>Senegal</code></li> <li>Serbia - <code>Serbia</code></li> <li>Seychelles - <code>Seychelles</code></li> <li>Singapore - <code>Singapore</code></li> <li>Slovakia - <code>Slovakia</code></li> <li>Slovenia - <code>Slovenia</code></li> <li>Somalia - <code>Somalia</code></li> <li>South Africa - <code>SouthAfrica</code></li> <li>South Korea - <code>SouthKorea</code></li> <li>Spain - <code>Spain</code></li> <li>Sri Lanka - <code>SriLanka</code></li> <li>Sudan - <code>Sudan</code></li> <li>Suriname - <code>Suriname</code></li> <li>Sweden - <code>Sweden</code></li> <li>Switzerland - <code>Switzerland</code></li> <li>Syria - <code>Syria</code></li> <li>Taiwan - <code>Taiwan</code></li> <li>Tajikistan - <code>Tajikistan</code></li> <li>Thailand - <code>Thailand</code></li> <li>Trinidad and Tobago - <code>TrinidadandTobago</code></li> <li>Tunisia - <code>Tunisia</code></li> <li>Turkey - <code>Turkey</code></li> <li>Uganda - <code>Uganda</code></li> <li>Ukraine - <code>Ukraine</code></li> <li>United Arab Emirates - <code>UnitedArabEmirates</code></li> <li>United Kingdom - <code>UnitedKingdom</code></li> <li>United States - <code>UnitedStates</code></li> <li>Uzbekistan - <code>Uzbekistan</code></li> <li>Venezuela - <code>Venezuela</code></li> <li>Vietnam - <code>Vietnam</code></li> <li>Zambia - <code>Zambia</code></li> </ul>"},{"location":"features/custom-browser/","title":"Custom browser","text":""},{"location":"features/custom-browser/#custom-browser-usage","title":"Custom browser usage","text":"<p>You can setup playwright browser with custom parameters for development puposes and use it with Parsera!</p> <pre><code>    async with async_playwright() as p:\n        browser = await p.firefox.launch(headless=False, slow_mo=100)\n        loader = PageLoader(browser=browser)\n        await loader.create_session()\n        content = await loader.fetch_page(url=url)\n        return content\n</code></pre> <p>In this example you can try how to use browser with the custom options such as slow mode and window mode.</p> <p>Check out full example</p>"},{"location":"features/custom-cookies/","title":"Custom cookies","text":""},{"location":"features/custom-cookies/#custom-cookies-usage","title":"Custom cookies usage","text":"<p>To manage sessions more effectively, you can supply your session cookies directly to Parsera.</p>"},{"location":"features/custom-cookies/#cookies-format-and-known-issues","title":"Cookies format and known issues","text":"<p>Please provide cookies to Parsera in JSON format, as shown in the example below.</p> <p>To retrieve session cookies from your browser, you can use various browser plugins. However, be aware that some plugins may export cookies in an incorrect format, which can lead to issues.</p> <p>Note: Pay special attention to the SameSite attribute. Valid values for SameSite are Strict, Lax, or None. Some plugins may incorrectly set this attribute to other values, which could cause parsing issues in Parsera.</p> <p>Example: <pre><code>    file_with_cookeis = \"cookies.json\"\n    with open(file_with_cookeis, \"r\") as file:\n        cookies = json.load(file)\n\n    parsera = Parsera(model=model, custom_cookies=cookies)\n    return await parsera.arun(\n        url=\"https://www.reddit.com/settings/\",\n        elements={\n            \"email address\": \"value\",\n        },\n    )\n</code></pre> Try out full example</p>"},{"location":"features/custom-models/","title":"Custom models","text":""},{"location":"features/custom-models/#run-with-custom-model","title":"Run with custom model","text":"<p>You can instantiate <code>Parsera</code> with any chat model supported by LangChain, for example, to run the model from Azure: <pre><code>import os\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.getenv(\"AZURE_GPT_BASE_URL\"),\n    openai_api_version=\"2023-05-15\",\n    deployment_name=os.getenv(\"AZURE_GPT_DEPLOYMENT_NAME\"),\n    openai_api_key=os.getenv(\"AZURE_GPT_API_KEY\"),\n    openai_api_type=\"azure\",\n    temperature=0.0,\n)\n\nurl = \"https://news.ycombinator.com/\"\nelements = {\n    \"Title\": \"News title\",\n    \"Points\": \"Number of points\",\n    \"Comments\": \"Number of comments\",\n}\nscrapper = Parsera(model=llm)\nresult = scrapper.run(url=url, elements=elements)\n</code></pre></p>"},{"location":"features/custom-models/#run-local-model-with-trasformers","title":"Run local model with <code>Trasformers</code>","text":"<p>Currently, we only support models that include a <code>system</code> token</p> <p>You should install <code>Transformers</code> with either <code>pytorch</code> (recommended) or <code>TensorFlow 2.0</code></p> <p>Transformers Installation Guide</p> <p>Example: <pre><code>from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom parsera.engine.model import HuggingFaceModel\nfrom parsera import Parsera\n\n# Define the URL and elements to scrape\nurl = \"https://news.ycombinator.com/\"\nelements = {\n\"Title\": \"News title\",\n\"Points\": \"Number of points\",\n\"Comments\": \"Number of comments\",\n}\n\n# Initialize model with transformers pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", trust_remote_code=True)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=5000)\n\n# Initialize HuggingFaceModel\nllm = HuggingFaceModel(pipeline=pipe)\n\n# Scrapper with HuggingFace model\nscrapper = Parsera(model=llm)\nresult = scrapper.run(url=url, elements=elements)\n</code></pre></p>"},{"location":"features/custom-playwright/","title":"Custom playwright","text":""},{"location":"features/custom-playwright/#scripting-with-parsera","title":"Scripting with Parsera","text":"<p>With <code>Parsera</code> class you can execute custom playwright scripts during scraping. There are 2 types of code you can run:</p> <ul> <li><code>initial_script</code> which is executed during the first run of <code>Parsera</code>, useful when you need to log in to access the data.</li> <li><code>playwright_script</code> which runs during every <code>run</code> call, which allows to do custom actions before data is extracted, useful when data is hidden behind some button.</li> </ul>"},{"location":"features/custom-playwright/#example-log-in-and-load-data","title":"Example: log in and load data","text":"<p>You can log in to parsera.org and get credits amount with the following code: <pre><code>from playwright.async_api import Page\nfrom parsera import Parsera\n\n# Define the script to execute during the session creation\nasync def initial_script(page: Page) -&gt; Page:\n    await page.goto(\"https://parsera.org/auth/sign-in\")\n    await page.wait_for_load_state(\"networkidle\")\n    await page.get_by_label(\"Email\").fill(EMAIL)\n    await page.get_by_label(\"Password\").fill(PASSWORD)\n    await page.get_by_role(\"button\", name=\"Sign In\", exact=True).click()\n    await page.wait_for_selector(\"text=Playground\")\n    return page\n\n# This script is executed after the url is opened\nasync def repeating_script(page: Page) -&gt; Page:\n    await page.wait_for_timeout(1000)  # Wait one second for page to load\n    return page\n\nparsera = Parsera(model=model, initial_script=initial_script)\nresult = await parsera.arun(\n    url=\"https://parsera.org/app\",\n    elements={\n        \"credits\": \"number of credits\",\n    },\n    playwright_script=repeating_script,\n)\n</code></pre></p>"},{"location":"features/custom-playwright/#access-playwright-instance","title":"Access Playwright instance","text":"<p>The page is fetched via the <code>Parsera.loader</code>, which contains the playwright instance. <pre><code>from parsera import Parsera\n\nparsera = Parsera(model=model)\n\n## You can manually initialize playwright session and modify it:\nawait parsera.loader.create_session()\nawait parsera.loader.fetch_page(url=url)\n\n## After page is loaded you can access playwright elements, like Page:\nparsera.loader.page.getByRole('button').click()\n\n## Next you cun run extraction process\nresult = await parsera.arun(\n    url=extraction_url,\n    elements=elements_dict,\n)\n</code></pre></p>"},{"location":"features/docker/","title":"Docker","text":""},{"location":"features/docker/#running-in-docker","title":"Running in Docker","text":"<p>You can get access to the CLI or development environment using Docker.</p>"},{"location":"features/docker/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker: Install Docker</li> <li>Docker Compose: Install Docker Compose</li> </ul>"},{"location":"features/docker/#quickstart","title":"Quickstart","text":"<ol> <li>Create a .env file in the project root directory with the following content:</li> </ol> <pre><code>URL=https://parsera.org\nFILE=/app/scheme.json\nOUTPUT=/app/output/result.json\nSCROLLS=5\n</code></pre> <ol> <li> <p>Create <code>scheme.json</code> file with the parsing scheme in the repository root directory.</p> </li> <li> <p>Run <code>make up</code> in this directory.</p> </li> <li> <p>The output will be saved as <code>output/results.json</code> file.</p> </li> </ol>"},{"location":"features/docker/#docker-make-targets","title":"Docker Make Targets","text":"<pre><code>make build # Build Docker image\n\nmake up # Start containers using Docker Compose\n\nmake down # Stop and remove containers using Docker Compose\n\nmake restart # Restart containers using Docker Compose\n\nmake logs # View logs of the containers\n\nmake shell # Open a shell in the running container\n\nmake clean # Remove all stopped containers, unused networks, and dangling images\n</code></pre>"},{"location":"features/extractors/","title":"Extractors","text":""},{"location":"features/extractors/#different-extractor-types","title":"Different extractor types","text":"<p>There are different types of extractors, that provide output in different formats:</p> <ul> <li>For tables.<ul> <li><code>ChunksTabularExtractor</code> - for tables, capable of processing larger pages with chunking</li> <li><code>TabularExtractor</code> - for tables, without chunking (fails when page doesn't fit into the model's context)</li> </ul> </li> <li><code>ListExtractor</code> for separate lists of values.</li> <li><code>ItemExtractor</code> for specific values.</li> </ul> <p>By default a <code>ChunksTabularExtractor</code> is used.</p>"},{"location":"features/extractors/#tabular-extractor","title":"Tabular Extractor","text":"<p><pre><code>from parsera import Parsera\nfrom parsera.engine.simple_extractor import TabularExtractor\n\nextractor = TabularExtractor()\nscraper = Parsera(extractor=extractor)\n</code></pre> The tabular extractor is used to find rows of tabular data and has output of the form: <pre><code>[\n    {\"name\": \"name1\", \"price\": \"100\"},\n    {\"name\": \"name2\", \"price\": \"150\"},\n    {\"name\": \"name3\", \"price\": \"300\"},\n]\n</code></pre></p>"},{"location":"features/extractors/#chunks-tabular-extractor","title":"Chunks Tabular Extractor","text":"<p>Provides the same output format as <code>TabularExtractor</code>, but capable of processing larger pages due to page chunking. For example, if your model has 16k context size, you can set chunks to be not larger than 12k (keeping 4k buffer for other parts of the prompt): <pre><code>from parsera import Parsera\nfrom parsera.engine.chunks_extractor import ChunksTabularExtractor\n\nextractor = ChunksTabularExtractor(chunk_size=12000)\nscraper = Parsera(extractor=extractor)\n</code></pre></p> <p>By default number of tokens is counted based on the OpenAI tokenizer for <code>gpt-4o</code> model, but you can provide custom function for counting tokens:</p> <pre><code>import tiktoken\n\ndef count_tokens(text):\n    # Initialize the tokenizer for GPT-4o-mini\n    encoding = tiktoken.get_encoding(\"cl100k_base\")\n\n    # Count tokens\n    tokens = encoding.encode(text)\n    return len(tokens)\n\nscraper = Parsera(extractor=ExtractorType.CHUNKS_TABULAR, chunk_size=12000, token_counter=count_tokens)\n</code></pre>"},{"location":"features/extractors/#list-extractor","title":"List Extractor","text":"<p><pre><code>from parsera import Parsera\nfrom parsera.engine.simple_extractor import ListExtractor\n\nextractor = ListExtractor()\nscraper = Parsera(extractor=extractor)\n</code></pre> The list extractor is used to find lists of different values and has output of the form: <pre><code>{\n    \"name\": [\"name1\", \"name2\", \"name3\"],\n    \"price\": [\"100\", \"150\", \"300\"]\n}\n</code></pre></p>"},{"location":"features/extractors/#item-extractor","title":"Item Extractor","text":"<p><pre><code>from parsera import Parsera\nfrom parsera.engine.simple_extractor import ItemExtractor\n\nextractor = ItemExtractor()\nscraper = Parsera(extractor=extractor)\n</code></pre> The item extractor is used to get singular items from a page like a title or price and has output of the form: <pre><code>{\n    \"name\": \"name1\",\n    \"price\": \"100\"\n}\n</code></pre></p>"},{"location":"features/proxy/","title":"Proxy","text":""},{"location":"features/proxy/#using-proxy","title":"Using proxy","text":"<p>You can use serve the traffic via proxy server when calling <code>run</code> method: <pre><code>from parsera import Parsera\n\nscraper = Parsera()\nproxy_settings = {\n    \"server\": \"https://1.2.3.4:5678\",\n    \"username\": &lt;PROXY_USERNAME&gt;,\n    \"password\": &lt;PROXY_PASSWORD&gt;,\n}\nresult = scraper.run(url=url, elements=elements, proxy_settings=proxy_settings)\n</code></pre></p> <p>Where <code>proxy_settings</code> contains your proxy credentials.</p>"},{"location":"features/scrolling/","title":"Scrolling","text":""},{"location":"features/scrolling/#page-scrolling","title":"Page scrolling","text":"<p>Parsera library can scroll pages now!. To do this you simply should set parameter <code>scrolls_limit</code>.</p> <p>This parameters is available for the <code>run</code> and <code>arun</code> methods in the <code>Parsera</code> class instance.</p> <p>Check out the example below!: <pre><code>async def get_reddit_info():\n    model = GPT4oMiniModel()\n\n    parsera = Parsera(model=model)\n    return await parsera.arun(\n        url=\"https://www.reddit.com/\",\n        elements={\n            \"post name\": \"post description\"\n        },\n        scrolls_limit = 10\n    )\n</code></pre></p>"}]}